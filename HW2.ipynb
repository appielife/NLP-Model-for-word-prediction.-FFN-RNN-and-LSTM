{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appielife/NLP-Model-for-word-prediction.-FFN-RNN-and-LSTM/blob/master/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMDIMW_1Dyrc",
        "colab_type": "text"
      },
      "source": [
        "#Introduction \n",
        "\n",
        "### In this assignment, we ask you to build neural language models with recurrent neural networks. We provide the starter code for you. You need to implement RNN models here and write a separate report describing your experiments. Simply copying from other sources will be considered as plagiarism."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd6dr65WG_kh",
        "colab_type": "text"
      },
      "source": [
        "# Tasks\n",
        "\n",
        "*   [**Task 1.1: 10 Pts**] Additional data processing (5 Pts) with comments (5 Pts).\n",
        "\n",
        "*   [**Task 1.2: 50 Pts**] Complete this end-to-end pipeline with RNN and LSTM architectures and save the best model object for autograding (40 Pts). Clearly comment and explain your code using **+ Text** functionality in Colab (10 Pts).\n",
        "*   [**Task 2: 20 Pts**] Hyper-parameters tuning using the validation set. You need to write a separate report describing your experiments by tuning three hyper-parameters. See more details in Task 3.\n",
        "*   [**Task 3: 20 Pts**] Submit the best model object and class to Vocareum for grading. \n",
        "*   [**Task 4: Extra Credits**] Try adding addtional linguistic features or other DL architectures to improve your model: char-RNN, attention mechanisum, etc. You have to implement these models in the framework we provide and clearly comment your code. \n",
        "\n",
        "\n",
        "**Simply copying from other sources will be considered as plagiarism.** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeTVJCiPLIfY",
        "colab_type": "text"
      },
      "source": [
        "# Download Data and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dKfdbsQDgJh",
        "colab_type": "code",
        "outputId": "b3db9eab-eb7f-4d5f-8325-19b1bfe5a760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "! git clone https://github.com/rujunhan/CSCI-544.git\n",
        "# Install Tokenizer\n",
        "! pip install mosestokenizer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'CSCI-544' already exists and is not an empty directory.\n",
            "Requirement already satisfied: mosestokenizer in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: openfile in /usr/local/lib/python3.6/dist-packages (from mosestokenizer) (0.0.7)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from mosestokenizer) (0.6.2)\n",
            "Requirement already satisfied: toolwrapper in /usr/local/lib/python3.6/dist-packages (from mosestokenizer) (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG853h6XTyia",
        "colab_type": "text"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_G_EyxwSeYQ",
        "colab_type": "code",
        "outputId": "6e39db62-7b2d-4741-ce34-ef69dbcbeb45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from pathlib import Path\n",
        "from tqdm import tqdm \n",
        "from mosestokenizer import MosesTokenizer\n",
        "import logging as log\n",
        "\n",
        "log.basicConfig(level=log.INFO)\n",
        "tokr = MosesTokenizer()\n",
        "def read_tokenized(dir):\n",
        "  \"\"\"Tokenization wrapper\"\"\"\n",
        "  inputfile = open(dir)\n",
        "  for sent in inputfile:\n",
        "    yield tokr(sent.strip())\n",
        "  \n",
        "train_file = Path('train.txt')\n",
        "with train_file.open('w') as w:\n",
        "  for toks in tqdm(read_tokenized(Path('CSCI-544/hw2/train.txt'))):   \n",
        "    toks=list(map(lambda x:'<num>' if x.isdigit() else x.lower(), toks))\n",
        "#     log.info(f'{toks}') \n",
        "    w.write('<bos> '+\" \".join(toks) + ' <eos>\\n')\n",
        "    \n",
        "dev_file = Path('dev.txt')\n",
        "with dev_file.open('w') as w:\n",
        "  for toks in tqdm(read_tokenized(Path('CSCI-544/hw2/dev.txt'))):\n",
        "    toks=list(map(lambda x:'<num>' if x.isdigit() else x.lower(), toks))\n",
        "    w.write(\" \".join(toks) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:mosestokenizer.tokenizer.MosesTokenizer:executing argv ['perl', '/usr/local/lib/python3.6/dist-packages/mosestokenizer/tokenizer-v1.1.perl', '-q', '-l', 'en', '-b', '-a']\n",
            "INFO:mosestokenizer.tokenizer.MosesTokenizer:spawned process 360\n",
            "144526it [00:22, 6303.10it/s]\n",
            "36131it [00:05, 6332.06it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62ScMVlkauQZ",
        "colab_type": "code",
        "outputId": "7c818eaf-a07e-4d7a-a55e-81ae69dfa088",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "! ls -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 32132\n",
            "-rw-r--r-- 1 root root  6123832 Oct 21 20:49 best_model.pt\n",
            "drwxr-xr-x 4 root root     4096 Oct 21 19:37 CSCI-544\n",
            "-rw-r--r-- 1 root root  4984320 Oct 21 20:57 dev.txt\n",
            "drwxr-xr-x 1 root root     4096 Aug 27 16:17 sample_data\n",
            "-rw-r--r-- 1 root root 21694324 Oct 21 20:57 train.txt\n",
            "-rw-r--r-- 1 root root    84712 Oct 21 19:37 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PPmEUgw7xgl",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81Hqh_8oXG-d",
        "colab_type": "code",
        "outputId": "77fc5feb-e02b-4ce2-8e20-630cf0e897cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "! head train.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bos> but , while deliberation is taking place on how much europe should retreat from its historical domination of multilateral bodies , there is little vision beyond this . <eos>\n",
            "<bos> the kyoto protocol allows countries to meet their target reductions of co2 emissions by substituting bio @-@ fuels for fossil fuels . <eos>\n",
            "<bos> this independence was at the core of hegel &apos;s insistence that supporting oneself by earning a living is one of the key ways that we gain a sense of ourselves as individuals . <eos>\n",
            "<bos> but it can no longer be taken for granted that other eu countries will automatically ratify the agreements that they reach between themselves as the lodestar for common policies . <eos>\n",
            "<bos> there are serious risks . <eos>\n",
            "<bos> given their large foreign @-@ exchange reserves , we believe the time to begin such an initiative is now . <eos>\n",
            "<bos> iran is believed by many to be trying to develop one . <eos>\n",
            "<bos> indeed , the problem today is not excessive capital inflows ; international markets have largely turned against emerging markets . <eos>\n",
            "<bos> but this time they are likely to face a third alternative : mayawati , whose bahujan samaj party ( bsp ) may command a bloc of at least <num> seats . <eos>\n",
            "<bos> even among the sunnis and shias there are further divisions . <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkuxfhaRXb_u",
        "colab_type": "text"
      },
      "source": [
        "# Task 1.1: additional data processing [5 pts + 5 pts comments]\n",
        "\n",
        "Modify the above data processing code by\n",
        "\n",
        "1.   making all tokens lower case\n",
        "2.   mapping all numbers to a special symbol $\\langle num\\rangle$\n",
        "3.   adding $\\langle bos\\rangle$ and $\\langle eos\\rangle$ to the beginning and the end of a sentence\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpWN_IVCBTI4",
        "colab_type": "text"
      },
      "source": [
        "### NOTE \n",
        "MAX_TYPES, MIN_FREQ and BATCH_SIZE are fixed hyper-parameters for data. You are **NOT ALLOWED** to change these for fair comparison. The auto-grading script on Vocareum also uses these fixed values, so make sure you don't change them. We will ask you to experiment with other hyper-parameters related to model and report results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1-kpAyx-gMf",
        "colab_type": "code",
        "outputId": "aa098c5b-bf98-418d-9d7e-19307849e528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from typing import List, Iterator, Set, Dict, Optional, Tuple\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "RESERVED = ['<pad>', '<unk>']\n",
        "\n",
        "PAD_IDX = 0 \n",
        "UNK_IDX = 1\n",
        "MAX_TYPES = 10_000\n",
        "BATCH_SIZE = 256\n",
        "MIN_FREQ = 5\n",
        "\n",
        "class Vocab:\n",
        "  \"\"\" Mapper of words <--> index \"\"\"\n",
        "\n",
        "  def __init__(self, types):\n",
        "    # types is list of strings\n",
        "    assert isinstance(types, list)\n",
        "    assert isinstance(types[0], str)\n",
        "\n",
        "    self.idx2word = types\n",
        "    self.word2idx = {word: idx for idx, word in enumerate(types)}\n",
        "    assert len(self.idx2word) == len(self.word2idx)  # One-to-One\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.idx2word)\n",
        "  \n",
        "  def save(self, path: Path):\n",
        "    log.info(f'Saving vocab to {path}')\n",
        "    with path.open('w') as wr:\n",
        "      for word in self.idx2word:\n",
        "        wr.write(f'{word}\\n')\n",
        " \n",
        "  @staticmethod\n",
        "  def load(path):\n",
        "    log.info(f'loading vocab from {path}')\n",
        "    types = [line.strip() for line in path.open()]\n",
        "    for idx, tok in enumerate(RESERVED): # check reserved\n",
        "      assert types[idx] == tok\n",
        "    return Vocab(types)\n",
        "\n",
        "  @staticmethod\n",
        "  def from_text(corpus: Iterator[str], max_types: int,\n",
        "                             min_freq: int = 5):\n",
        "    \"\"\"\n",
        "    corpus: text corpus; iterator of strings\n",
        "    max_types: max size of vocabulary\n",
        "    min_freq: ignore word types that have fewer ferquency than this number\n",
        "    \"\"\"\n",
        "    log.info(\"building vocabulary; this might take some time\")\n",
        "    term_freqs = Counter(tok for line in corpus for tok in line.split())\n",
        "    for r in RESERVED:\n",
        "      if r in term_freqs:\n",
        "        log.warning(f'Found reserved word {r} in corpus')\n",
        "        del term_freqs[r]\n",
        "    term_freqs = list(term_freqs.items())\n",
        "    log.info(f\"Found {len(term_freqs)} types; given max_types={max_types}\")\n",
        "    term_freqs = {(t, f) for t, f in term_freqs if f >= min_freq}\n",
        "    log.info(f\"Found {len(term_freqs)} after dropping freq < {min_freq} terms\")\n",
        "    term_freqs = sorted(term_freqs, key=lambda x: x[1], reverse=True)\n",
        "    term_freqs = term_freqs[:max_types]\n",
        "    types = [t for t, f in term_freqs]\n",
        "    types = RESERVED + types   # prepend reserved words\n",
        "    return Vocab(types)\n",
        "\n",
        "\n",
        "train_file = Path('train.txt')\n",
        "vocab_file = Path('vocab.txt')\n",
        "\n",
        "if not vocab_file.exists():\n",
        "  train_corpus = (line.strip() for line in train_file.open())\n",
        "  vocab = Vocab.from_text(train_corpus, max_types=MAX_TYPES, min_freq=MIN_FREQ)\n",
        "  vocab.save(vocab_file)\n",
        "else:\n",
        "  vocab = Vocab.load(vocab_file)\n",
        "\n",
        "log.info(f'Vocab has {len(vocab)} types')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:loading vocab from vocab.txt\n",
            "INFO:root:Vocab has 10002 types\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnUbc7QlEC_k",
        "colab_type": "code",
        "outputId": "c92e2bb1-2b36-4671-abe9-17f454408b45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import copy\n",
        "\n",
        "class TextDataset:\n",
        "\n",
        "  def __init__(self, vocab: Vocab, path: Path):\n",
        "    self.vocab = vocab\n",
        "    log.info(f'loading data from {path}')\n",
        "    # for simplicity, loading everything to memory; on large datasets this will cause OOM\n",
        "\n",
        "    text = [line.strip().split() for line in path.open()]\n",
        "\n",
        "    # words to index; out-of-vocab words are replaced with UNK\n",
        "    xs = [[self.vocab.word2idx.get(tok, UNK_IDX) for tok in tokss] \n",
        "                 for tokss in text]\n",
        "    \n",
        "    self.data = xs\n",
        "    \n",
        "    log.info(f\"Found {len(self.data)} records in {path}\")\n",
        "\n",
        "  def as_batches(self, batch_size, shuffle=False): # data already shuffled\n",
        "    data = self.data\n",
        "    if shuffle:\n",
        "      random.shuffle(data)\n",
        "    for i in range(0, len(data), batch_size): # i incrememt by batch_size\n",
        "      batch = data[i: i + batch_size]  # slice\n",
        "      yield self.batch_as_tensors(batch)\n",
        "  \n",
        "  @staticmethod\n",
        "  def batch_as_tensors(batch):\n",
        "    \n",
        "    n_ex = len(batch)\n",
        "    max_len = max(len(seq) for seq in batch)\n",
        "    seqs_tensor = torch.full(size=(n_ex, max_len), fill_value=PAD_IDX,\n",
        "                             dtype=torch.long)\n",
        "    \n",
        "    for i, seq in enumerate(batch):\n",
        "      seqs_tensor[i, 0:len(seq)] = torch.tensor(seq)\n",
        "      \n",
        "    return seqs_tensor\n",
        "\n",
        "train_data = TextDataset(vocab=vocab, path=train_file)\n",
        "log.info(f'train_data{train_data}')\n",
        "dev_data = TextDataset(vocab=vocab, path=Path('dev.txt'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:loading data from train.txt\n",
            "INFO:root:Found 144526 records in train.txt\n",
            "INFO:root:train_data<__main__.TextDataset object at 0x7fa615acf2e8>\n",
            "INFO:root:loading data from dev.txt\n",
            "INFO:root:Found 36131 records in dev.txt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRWTb_C6IaCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "class FNN_LM(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, n_class, emb_dim=50, hid=100, dropout=0.2):\n",
        "    super(FNN_LM, self).__init__()\n",
        "    self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                  embedding_dim=emb_dim, \n",
        "                                  padding_idx=PAD_IDX)\n",
        "    self.linear1 = nn.Linear(emb_dim, hid)\n",
        "    self.linear2 = nn.Linear(hid, n_class)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self, seqs, log_probs=True):\n",
        "    \"\"\"Return log Probabilities\"\"\"\n",
        "    batch_size, max_len = seqs.shape\n",
        "    embs = self.embedding(seqs)  # embs[Batch x SeqLen x EmbDim]\n",
        "    embs = self.dropout(embs)\n",
        "    embs = embs.sum(dim=1)   # sum over all all steps in seq    \n",
        "    \n",
        "    hid_activated = torch.relu(self.linear1(embs)) # Non linear\n",
        "    scores = self.linear2(hid_activated)\n",
        "\n",
        "    if log_probs:\n",
        "      return torch.log_softmax(scores, dim=1)\n",
        "    else:\n",
        "      return torch.softmax(scores, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nwWlIZVZ1k2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model_object(model):\n",
        "  torch.save({'state_dict': model.state_dict()}, \"best_model.pt\")\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awWLp9XxPWf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "class RNN_LM(nn.Module):\n",
        "    def __init__(self, vocab_size, n_class, emb_dim=75, hid=100, num_layers=1, dropout=0.3):\n",
        "        super(RNN_LM, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                  embedding_dim=emb_dim, padding_idx=PAD_IDX)\n",
        "        self.linear1 = nn.Linear(emb_dim, hid)\n",
        "        self.linear2 = nn.Linear(hid, n_class)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.hid=hid\n",
        "        self.rnn = nn.RNN(emb_dim, hid, num_layers,dropout=dropout, batch_first=True)\n",
        "        self.n_class=n_class\n",
        "        \n",
        "        \n",
        "       \n",
        "    def forward(self, seqs):\n",
        "        batch_size, max_len = seqs.shape      \n",
        "        embs=self.embedding(seqs)\n",
        "  \n",
        "        embs=self.dropout(embs)\n",
        "        output,hidden = self.rnn(embs)\n",
        "#         print('Output1',output.size())\n",
        "#         print('hidden',hidden.size())\n",
        "        output=self.linear2(output[:,-1,:])         \n",
        "        return torch.log_softmax(output, dim=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Kh4X-5hi3g",
        "colab_type": "code",
        "outputId": "b6cd01ee-6ec8-43d2-a1fd-ae265aa4b017",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# Trainer Optimizer \n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "def train(model, n_epochs, batch_size, train_data, valid_data, device=torch.device('cuda')):\n",
        "  log.info(f\"Moving model to {device}\")\n",
        "  model = model.to(device)   # move model to desired device \n",
        "  optimizer = optim.Adam(params=model.parameters())\n",
        "  log.info(f\"Device for training {device}\")\n",
        "  losses = []\n",
        "  for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    num_toks = 0\n",
        "    train_loss = 0.\n",
        "    n_train_batches = 0\n",
        "\n",
        "    model.train() # switch to train mode \n",
        "    with tqdm(train_data.as_batches(batch_size=BATCH_SIZE), leave=False) as data_bar:\n",
        "      for seqs in data_bar:\n",
        "        seq_loss = torch.zeros(1).to(device)\n",
        "        for i in range(1, seqs.size()[1]-1):\n",
        "          # Move input to desired device\n",
        "          cur_seqs = seqs[:, :i].to(device) # take w0...w_(i-1) python indexing\n",
        "          cur_tars = seqs[:, i].to(device)  # predict w_i\n",
        "\n",
        "          log_probs = model(cur_seqs)\n",
        "          seq_loss += loss_func(log_probs, cur_tars).sum() / len(seqs)\n",
        "        \n",
        "        seq_loss /= (seqs.shape[1] - 1) # only n-1 toks are predicted\n",
        "        train_loss += seq_loss.item()\n",
        "        n_train_batches += 1\n",
        "\n",
        "        optimizer.zero_grad()         # clear grads\n",
        "        seq_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar_msg = f'Loss:{seq_loss.item():.4f}'\n",
        "        data_bar.set_postfix_str(pbar_msg)\n",
        "\n",
        "    # Run validation\n",
        "    with torch.no_grad():\n",
        "      model.eval() # switch to inference mode -- no grads, dropouts inactive\n",
        "      val_loss = 0\n",
        "      n_val_batches = 0\n",
        "      for seqs in valid_data.as_batches(batch_size=batch_size, shuffle=False):\n",
        "        # Move input to desired device\n",
        "        seq_loss = torch.zeros(1).to(device)\n",
        "        for i in range(1, seqs.size()[1]-1):\n",
        "          # Move input to desired device\n",
        "          cur_seqs = seqs[:, :i].to(device)\n",
        "          cur_tars = seqs[:, i].to(device)\n",
        "\n",
        "          log_probs = model(cur_seqs)\n",
        "          seq_loss += loss_func(log_probs, cur_tars).sum() / len(seqs)\n",
        "        seq_loss /= (seqs.shape[1] - 1)\n",
        "        val_loss += seq_loss.item() \n",
        "        n_val_batches += 1\n",
        "        \n",
        "    save_model_object(model)  \n",
        "    \n",
        "    avg_train_loss = train_loss / n_train_batches\n",
        "    avg_val_loss = val_loss / n_val_batches\n",
        "    losses.append((epoch, avg_train_loss, avg_val_loss))\n",
        "    log.info(f\"Epoch {epoch} complete; Losses: Train={avg_train_loss:G} Valid={avg_val_loss:G}\")\n",
        "  return losses\n",
        "\n",
        "# model = FNN_LM(vocab_size=len(vocab), n_class=len(vocab))\n",
        "model = RNN_LM(vocab_size=len(vocab), n_class=len(vocab))\n",
        "# model = LSTM_LM(vocab_size=len(vocab), n_class=len(vocab))\n",
        "\n",
        "\n",
        "\n",
        "loss_func = nn.NLLLoss(reduction='none')\n",
        "losses = train(model, n_epochs=6, batch_size=BATCH_SIZE, train_data=train_data,\n",
        "                valid_data=dev_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "INFO:root:Moving model to cuda\n",
            "INFO:root:Device for training cuda\n",
            "INFO:root:Epoch 0 complete; Losses: Train=2.37577 Valid=2.04495\n",
            "INFO:root:Epoch 1 complete; Losses: Train=1.85799 Valid=1.95045\n",
            "INFO:root:Epoch 2 complete; Losses: Train=1.79288 Valid=1.91861\n",
            "INFO:root:Epoch 3 complete; Losses: Train=1.75608 Valid=1.90898\n",
            "INFO:root:Epoch 4 complete; Losses: Train=1.73107 Valid=1.9118\n",
            "176it [02:09,  1.45it/s, Loss:1.2748]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGg2bW0QJQU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM_LM(nn.Module):\n",
        "    def __init__(self, vocab_size, n_class, emb_dim=5, hid=10, num_layers=1, dropout=0.2):\n",
        "        super(LSTM_LM, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                  embedding_dim=emb_dim, padding_idx=PAD_IDX)\n",
        "        self.linear2 = nn.Linear(hid, n_class)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.hid=hid\n",
        "        self.lstm = nn.LSTM(emb_dim, hid, num_layers,dropout=dropout, batch_first=True)\n",
        "        self.n_class=n_class\n",
        "        \n",
        "        \n",
        "       \n",
        "    def forward(self, seqs):\n",
        "        batch_size, max_len = seqs.shape      \n",
        "        embs=self.embedding(seqs)\n",
        "  \n",
        "        embs=self.dropout(embs)\n",
        "        output,(hidden,c) = self.lstm(embs)\n",
        "#         print('Output1',output.size())\n",
        "#         print('hidden',hidden.size())\n",
        "        output=self.linear2(output[:,-1,:])         \n",
        "        return torch.log_softmax(output, dim=1)\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7BiXswudYWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIAefv_QRbOK",
        "colab_type": "text"
      },
      "source": [
        "# Task 1.2: RNNs [50 pts]\n",
        "\n",
        "1.   Under the given FNN_LM framework, modify the code to implement RNN model [15 pts + 5 pts comments]\n",
        "2.   Repeat this step for LSTM model [15 pts + 5 pts comments]\n",
        "3.   Write a report comparing your results for these three models [10 pts]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXIMU22xSYRs",
        "colab_type": "text"
      },
      "source": [
        "# Task 2: Hyper-parameters Tuning [20 pts]\n",
        "You may observe that there are multiple hyper-parameters used in the pipeline. Choose 3 out of 5 following hyper-parameters and try at least 5 different values for each of them and report their corresponding performances on the train / dev datasets. Explain why you think larger or smaller values may cause the differenes you observed.\n",
        "\n",
        "1.   emb_dim: embedding size\n",
        "2.   hid: hidden layer dimension\n",
        "3.   num_layers: number of RNN layers\n",
        "4.   dropout ratio\n",
        "5.   n_epochs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il3G0V8bGQaY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96qH1owhU-mb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"best_model.pt\")\n",
        "files.download(\"vocab.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmEd-7gnfX9l",
        "colab_type": "text"
      },
      "source": [
        "#Task 3: Submitting your model class and object\n",
        "1. After you find the best model architecture, rename your best model as **BEST_MODEL** and re-run train() to save your model.\n",
        "2. Download model object and locate it (best_model.pt file) in your local direcotry and submit it to Vocareum.\n",
        "3. Copy your **BEST_MODEL** class into a python script: best_model.py and submit it to Vocareum.\n",
        "4. Download you **vocab.txt** file and submit it with your model files.\n",
        "\n",
        "In summary, you will need a **best_model.py** file,  a **best_model.pt** object and a **vocab.txt** file to successfully run the auto-grading on Vocareum.\n",
        "\n",
        "We made the evaluation code visible (but not editable) to everyone on Vocareum. You can find it here: resource/asnlib/public/evaluation.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXd-NhiO3jNz",
        "colab_type": "text"
      },
      "source": [
        "See below for an example. Rename FNN() class as BEST_MODEL. Modify and save the entire script below as best_model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irATbb9o3C24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "class BEST_MODEL(torch.nn.Module):\n",
        "  def __init__(self, vocab_size, n_class, emb_dim=50, hid=100, dropout=0.2):\n",
        "    super(BEST_MODEL, self).__init__()\n",
        "    self.embedding = torch.nn.Embedding(num_embeddings=vocab_size,\n",
        "                                embedding_dim=emb_dim,\n",
        "                                padding_idx=0)\n",
        "    self.linear1 = torch.nn.Linear(emb_dim, hid)\n",
        "    self.linear2 = torch.nn.Linear(hid, n_class)\n",
        "    self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self, seqs, log_probs=True):\n",
        "    \"\"\"Return log Probabilities\"\"\"\n",
        "    batch_size, max_len = seqs.shape\n",
        "    embs = self.embedding(seqs)  # embs[Batch x SeqLen x EmbDim]                                                                                                                                          \n",
        "    embs = self.dropout(embs)\n",
        "    embs = embs.sum(dim=1)   # sum over all all steps in seq                                             \\\n",
        "                                                                           \n",
        "    hid_activated = torch.relu(self.linear1(embs)) # Non linear                                                                                                                        \n",
        "    scores = self.linear2(hid_activated)\n",
        "\n",
        "    if log_probs:\n",
        "      return torch.log_softmax(scores, dim=1)\n",
        "    else:\n",
        "      return torch.softmax(scores, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdGo7roHRxC2",
        "colab_type": "text"
      },
      "source": [
        "# Task 4:  [Extra Credits 5Pts]\n",
        "\n",
        "Enhance the current model with additional linguistic features or DL models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSRJRwLByCfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}